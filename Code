#!/usr/bin/env python
# coding: utf-8

Using the dataset, create a Random Forest Classifier to classify patients with and without diabetes in the ‘outcome’ column.

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("assignment7.csv")
data.head()


# In[2]:


X = data.iloc[:, 0:8].values
y = data.iloc[:, 8].values

### split the test and training set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.25,
                                                    random_state=0)

### Scale the data
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)


# In[3]:


from sklearn.model_selection import GridSearchCV


# In[4]:


from sklearn.tree import DecisionTreeClassifier

### Run classification random forest
clf = DecisionTreeClassifier(max_depth = 3, random_state = 0)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)


# In[5]:


from sklearn import metrics
### Examin the prediction 
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:',
      np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))


# In[16]:


### Visiualize the result
from sklearn import tree
feature_names=data.columns[0:8]
feature_names=feature_names.to_numpy()
target_names=["Yes","No"]
tree.plot_tree(clf,feature_names=feature_names,class_names=target_names,filled=True)
plt.rcParams['figure.figsize'] = [50, 100]


# ##### Create another classification model using XGBoost for the same dataset.

# In[6]:


#! pip install xgboost


# In[7]:


import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import confusion_matrix


# In[8]:


xgbc = XGBClassifier()
#XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
#       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,
#       max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,
#       n_estimators=10, n_jobs=1, nthread=None,
#       objective='multi：softmax', random_state=0, reg_alpha=0,
#       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,
#       subsample=1, verbosity=1) 


# In[9]:


xgbc.fit(X_train, y_train)


# In[10]:


scores = cross_val_score(xgbc, X_train, y_train, cv=5)
print("Mean cross-validation score: %.2f" % scores.mean())


# In[11]:


kfold = KFold(n_splits=10, shuffle=True)
kf_cv_scores = cross_val_score(xgbc, X_train, y_train, cv=kfold )
print("K-fold CV average score: %.2f" % kf_cv_scores.mean())


# In[12]:


y_pred2 = xgbc.predict(X_test)
cm = confusion_matrix(y_test,y_pred2) 
print(cm)


# In[13]:


from sklearn.metrics import accuracy_score
from sklearn import metrics
accuracy = accuracy_score(y_test, y_pred2)
### Examin the prediction 
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred2))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred2))
print('Root Mean Squared Error:',
      np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

print("Accuracy: %.2f%%" % (accuracy * 100.0))


# In[15]:


from xgboost import plot_tree
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [50, 100]
plot_tree(xgbc,num_trees=2)
plt.show()


# #### Compare performance of both algorithms and give your observations as to which model performed better and give your reasoning as to why that model trained better. (In other words, for your observations compare strengths and weaknesses of both algorithms)

# In[17]:


#### Random Forest performance
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:',
      np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: %.2f%%" % (accuracy * 100.0))



#### XGboost performance
from sklearn.metrics import accuracy_score
from sklearn import metrics
accuracy = accuracy_score(y_test, y_pred2)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred2))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred2))
print('Root Mean Squared Error:',
      np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))

print("Accuracy: %.2f%%" % (accuracy * 100.0))
